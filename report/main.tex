
%% bare_jrnl.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/



% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[journal]{IEEEtran}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Restricted Boltzmann Machines}


\author{Jason~Liang,
        Julian~Sia% <-this % stops a space
% \thanks{M. Shell is with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
% \thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised January 11, 2007.}
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.

% make the title area
\maketitle

\begin{abstract}
In this report, we describe an undirected graphical model called Restricted Boltzmann Machine (RBM) that is capable of learning complex distributions. We analyze learning procedures and applications of RBMs and show experimental results for training RBMs on a standard image dataset.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
Restricted Boltzmann Machines (RBMs) are commonly used in classification, dimensionality reduction, and representation learning tasks. They are a type of undirected graphical model consisting of two sets of nodes, a set of hidden units and a set of visible units.  Given the graph $G = (V,E)$ that describes the RBM, $\forall \, v_{vis} \: \epsilon \: V_{vis} \subset V,\exists e \epsilon E$, such that $ e = (v_{vis}, v_{hid}),\: \forall \, v_{hid} \, \epsilon \, V_{hid} \subset V$.  RBMs are a variant of the Boltzmann Machine model \cite{ackley1985learning} posed by Hinton and Sejnowski, with the restriction that there are neither edges connecting any visible units nor edges connecting any hidden units \cite{hinton2006fast}. This restriction guarantees that the visible units are conditionally independent of other visible units; and hidden units, conditionally independent of hidden units. Furthermore, it makes determining the gradient of the free energy equation, which will be described in more detail later, tractable. The structure can be visually elucidated with the representation below. In Fig.~\ref{rbm}, we show an simple example RBM with a hidden layer of 3 units and a visible layer of 6 units.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{rbm.png}
  \caption{A graphical representation of a RBM illustrated by Cueto et al. \cite{cueto2009geometry}. Note that there are no connections in between the visible units $h$ and hidden units $v$.}
  \label{rbm}
\end{figure}

\section{Previous Work}

RBMs were created as a variant of Boltzmann Machines after practical difficulties were encountered in training Boltzmann Machines due to the fact that Boltzmann Machines are fully connected graphical models. Boltzmann Machine themselves were motivated primarily by limitations of Hopfield Networks \cite{hopfield1985neural}: their poor storage capacity and tendency to converge at local energy minima.  

In the early days of Boltzmann machines, Hinton and Sejnowski calculated the gradient of the log likelihood by ``[fixing] a training vector on the visible units, initializing the hidden units to random binary states, and using sequential Gibbs sampling of the hidden units" \cite{ackley1985learning}, using simulated annealing to speed up the convergence process.  Other attempts were made by Neal with persistent Markov chains \cite{neal1992connectionist} and Peterson and Anderson with mean-field methods (instead of Gibbs Sampling) to speed up the learning process \cite{peterson1987mean}. After RBMs were posed as a simplifying restriction by Smolensky, Hinton discovered a way to speed up the learning process, coined contrastive divergence (CD) which is described in this report \cite{carreira2005contrastive,hinton2006fast,hinton2010practical}.  

\section{Algorithm Description}
Given a set of training vectors, V, to train a RBM, one aims to maximize the average probability, $p(v), v \epsilon V$, where
\begin{equation}
p(v) = \frac{1}{Z} \sum\limits_{h} e^{-E(v,h)},\;
Z = \sum\limits_{v,h} e^{-E(v,h)}
\end{equation}

This is the same as trying to minimize the negative log probability of $p(v)$. One can accomplish this by taking the derivative of the log probability of $p(v)$ with respect to the weights defined in the energy function, $E(v,h)$ defined as

\begin{equation}
\begin{aligned}
E(v,h) &= \:- \sum\limits_{i \, \epsilon \, visible} a_{i}v_{k} \: - \sum\limits_{j \, \epsilon \, hidden} b_{j}h_{j} \: - \sum\limits_{i}\sum\limits_{j} v_{i}w_{i,j}h_{j}\\
&=\: -b'v - c'h - h'Wv
\end{aligned}
\end{equation} where $W$ is a weight matrix and $c$ and $b$ are bias vectors corresponding to the hidden and visible nodes respectively. Differentiating with respect to $w_{i,j}$, we find that the partial derivative reduces to the following:

\begin{equation}
\begin{aligned}
-\frac{\partial \log p(v)}{\partial w_{i,j}} & = {\langle v_{i} h_{j} \rangle}_{data} - {\langle v_{i} h_{j} \rangle}_{model} \\
\end{aligned}
\end{equation}

With this reduction the following learning rule with $\epsilon$ learning rate can be derived

\begin{equation}
\Delta w_{i,j} = \epsilon\langle{\langle v_{i} h_{j} \rangle}_{data} - {\langle v_{i} h_{j} \rangle}_{model}\rangle 
\end{equation} 

Alternatively, using free energy notation, the partial derivative can be defined as
\begin{equation}
\begin{aligned}
- \frac{\partial \log p(v)}{\partial \Theta} = \frac{\partial \mathcal{F}(v)}{\partial \Theta} - \sum\limits_{\tilde{v}}p(\tilde{v}) \frac{\partial \mathcal{F}(\tilde{v})}{\partial \Theta}
\end{aligned}
\end{equation} where we define free energy as 

\begin{equation}
\mathcal{F}(v) = -log \sum\limits_{h}e^{-E(v,h)}
\end{equation} 

Here, $\Theta$ represents the model parameters for the RBM which include the weights $W$, hidden node biases $b$ and visible node biases $c$. Because the computation of the second term in (5) is intractable, we can approximate the second term using a fixed number of samples from the model, such that 

\begin{equation}
\begin{aligned}
- \frac{\partial \log p(v)}{\partial \Theta} = \frac{\partial \mathcal{F}(v)}{\partial \Theta} -\frac{1}{|\mathcal{N}|} \sum\limits_{\tilde{v} \epsilon \mathcal{N}} p(\tilde{v}) \frac{\partial \mathcal{F}(\tilde{v})}{\partial \Theta}
\end{aligned}
\end{equation}

In the case of a binary RBM, the free energy function reduces to the following:

\begin{equation}
\begin{aligned}
\mathcal{F}(v) = -b'v - \sum\limits_{i}log\sum\limits_{h_{i}}e^{h_{i}(c_{i}+W_{i}v)}
\end{aligned}
\end{equation}

Using the free energy function defined in (8) and the derivative defined in (7), we can define the following update equations for the RBM's negative log likelihood gradient:

\begin{equation}
\begin{aligned}
- \frac{\partial p(v)}{\partial W_{i,j}} &= E_{v}[p(h_{i}|v)\cdot v_{j}] - v_{j}^{(i)}\cdot sigm(W_{i} \cdot v^{(i)} + c_{i})\\
- \frac{\partial p(v)}{\partial c_{i}} &= E_{v}[p(h_{i}|v)] - v_{j}^{(i)}\cdot sigm(W_{i} \cdot v^{(i)})\\
- \frac{\partial p(v)}{\partial b_{j}} &= E_{v}[p(v_{j}|h)] - v_{j}^{(i)}
\end{aligned}
\end{equation}

To obtain samples from p(v) and compute the gradient, one can treat the RBM as a Markov chain and perform Gibbs sampling by alternating between fixing the hidden nodes, sampling the visible nodes and vice-versa. We note the conditional independence assumptions for the hidden and visible layers:

\begin{equation}
\begin{aligned}
p(h|v) = \prod\limits_{i} p(h_{i}|v)\\
p(v|h) = \prod\limits_{j} p(v_{j}|h)
\end{aligned}
\end{equation} and also the conditional distributions for each visible and hidden node:

\begin{equation}
\begin{aligned}
p(h_{i} = 1|v) = sigm(c_{i} + W_{i}v)\\
p(v_{j} = 1|j) = sigm(b_{j} + W_{i}'h)
\end{aligned}
\end{equation} where $sigm(x)$ is the logistic sigmoid function.

As a consequence of (10) and (11), we can perform block Gibbs Sampling with the following update rules:

\begin{equation}
\begin{aligned}
h^{(n+1)} \sim sigm(W'v^{(n)} + c)\\
v^{(n+1)} \sim sigm(Wh^{(n+1)} + b)
\end{aligned}
\end{equation}

As $t \rightarrow \infty,  (v^{(t)},h^{(t)})$, $v^{t}$ and $h^{t}$ will be accurate samples drawn from the RBM's distribution. Nevertheless, one can further speed up the process by using the contrastive divergence (CD) algorithm, which computes an approximation of the gradient by performing Gibbs sampling for a finite number of steps. This involves initializing the Markov chain with a training example $v \epsilon V$ and running the Markov chain for k (often with $k = 1$) steps. In the context of the gradient defined in (7), CD can be roughly outlined as the following:

\begin{enumerate}
 \item Replace the first term (expectation over all input samples) with a single sample.
 \item For the second term, run the Markov chain for fixed $k$ steps.
\end{enumerate}

Thus, CD-1, which runs the Markov Chain for only 1 step, can be described as the following:

\begin{enumerate}
\item Take a training sample $v$, compute the probabilities of the hidden units and sample a hidden activation vector $h$ from this probability distribution.
\item Compute the outer product of $v$ and $h$ and call this the positive gradient.
\item From $h$, sample a reconstruction $v'$ of the visible units, then resample the hidden activations $h'$ from this. (Gibbs sampling step)
\item Compute the outer product of $v'$ and $h'$ and call this the negative gradient.
\item Let the weight update to $w_{i,j}$ be the positive gradient minus the negative gradient, times some learning rate (see below). 
\end{enumerate}

\begin{equation}
\begin{aligned}
\Delta w_{i,j} = \epsilon(vh^{T} - v'h'^{T})
\end{aligned}
\end{equation}

The bias updates for the visible and hidden layers respectively can be defined by these expressions:

\begin{equation}
\begin{aligned}
\Delta v_{b} = \epsilon(v - v')\\
\Delta h_{b} = \epsilon(h - h')\\
\end{aligned}
\end{equation}

CD-k is a generalization of CD-1 by repeating the sampling process in step three k times, though in practice it has been found that 1 iteration (CD-1) works fairly well in ``[ensuring] that hidden features retain most of the information in the data vector" \cite{hinton2010practical}.  One extension of contrastive divergence is persistent contrastive divergence (PCD). When given a new input sample for training, PCD computes the second term (the negative gradient) by continuing Gibbs sampling from the stopping position of the Markov chain of the previous input instead of rerunning from scratch \cite{tieleman2008training}. Experimental results show that it allows more accurate samples to be drawn from the RBM's current distribution. 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{reduce.jpg}
  \caption{Comparison of A) PCA and B) RBM in their ability to perform dimensionality reduction on the MNIST dataset \cite{hinton2006reducing}.}
  \label{reduce}
\end{figure}

\section{Applications of RBMs}
RBMs has been show to be capable of representation learning, also known as manifold learning. Most real world data are high dimensional, but they tend to lie close to a low dimensional manifold that is embedded in the high dimensional space. What RBMs are essentially doing is learning a mapping that transforms the original input data (high dimensional vectors) into coordinates along that manifold (low dimensional vectors). After an RBM has been trained, it can be reinterpreted as a deterministic encoder, a single layer neural network that performs dimensionality reduction. Other dimensionality reduction techniques such as PCA can only learn a linear manifold or hyperplane. However, RBMs with sufficient number of hidden units are capable of learning any arbitrary manifold shapes. As shown in Fig.~\ref{reduce}, RBMs are much more effective than PCA in performing dimensionality reduction on an example dataset such as MNIST \cite{deng2012mnist}. The different classes of digits are more visually distinct and separable when using RBMs instead of PCA to transform the digits. 

A popular application of RBMs that exploits their representation learning capabilities is stacking them to construct deep neural networks. After a RBM has been trained, the samples $h$ from the hidden layer can be used as input to train a second RBM. This can be extended to an arbitrary number of RBMs. After training multiple RBMs, they can be converted into a deep neural network by treating each RBM layer deterministically. A deep neural network of such variety can then be fine tuned with common supervised training methods such as backpropagation \cite{hinton2006reducing} and used for tasks like image classification. In 2006, Hinton used stacked RBMs to train a deep neural network that achieved an error rate of 1.39\%, which outperformed a SVM with RBF kernel, the state of the art at the time \cite{hinton2006fast}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{filters.png}
  \caption{Filters corresponding the weights of the first 100 hidden nodes.}
  \label{filters}
\end{figure}

\section{Experimental Results}
We implement our own RBM in Python with the NumPy library \cite{oliphant2006guide}. We train a RBM with 784 visible units and 500 hidden units on the MNIST handwritten digit dataset \cite{deng2012mnist}, a standard benchmark for image classification. The learning algorithm used is CD-1, which is the simplest of the CD methods but tends to work very well in practice. We initialize the weights of the RBM randomly by sampling them from a normal distribution with zero mean and 0.01 standard deviation. Next, we apply CD-1 to all 60,000 training images for 15 epochs with a learning rate 0.005.

In Fig.~\ref{filters}, we visualize the trained weights of the RBM. Each hidden node is connected to all 784 visible nodes and thus we can display the weights corresponding to a hidden node as an image with the same dimension as the input digits. We see that the filters learned are interesting, with a diverse variety of different edge and blob detectors. 

Next in Fig.~\ref{samples}, we show the samples generated by the RBM when given test digits as input. The first column shows the original digits while the subsequent columns shows samples outputted by the model after an interval of 1000 steps of Gibbs sampling (to ensure proper mixing of the Markov chain). The images in Fig.~\ref{samples} show that RBM has properly learned the distribution for input digits and that samples are drawn from that distribution properly. 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{samples.png}
  \caption{Samples generated by RBM. The first column shows original test digit sample. Subsequent columns show digits sampled from RBM with an interval of 1000 steps of Gibbs sampling.}
  \label{samples}
\end{figure}

Lastly, we explore the usefulness of RBMs in learning good representations of the input data and performing dimensionality reduction. As a preprocessing step prior to classification, we treat our trained RBM as a deterministic single layer neural network and use it to transform the input digits into a 500 dimensional feature vector. This feature vector is then used for supervised training of a logistic regression classifier. Without an RBM layer, the misclassification rate on a test set of 10,000 digits is 8.20\%. With the addition of the RBM layer, the error rate of the logistic regression classifier decreases to 3.07\%, a relative improvement of over 150\%. 

\section{Conclusion}

As demonstrated in previous research and in our experiments, Restricted Boltzmann Machines are capable of learning complex distributions and good representations of the input data. They are also useful as a preprocessing step that improves the classification accuracy for the input data. With their relatively simple training procedures and given recent innovations in the use of RBMs, they are widely applied in many problem domains and are a crucial step for building deep neural networks.




% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliographystyle{unsrt}
\bibliography{main}

% that's all folks
\end{document}
